Subject: [patch] mm: page_alloc: use vmstats for fair zone allocation batching

Avoid dirtying the same cache line with every single page allocation
by making the fair per-zone allocation batch a vmstat item, which will
turn it into batched percpu counters on SMP.

Signed-off-by: Johannes Weiner <[hidden email]>
---
 include/linux/mmzone.h |  2 +-
 mm/page_alloc.c        | 21 ++++++++++++---------
 mm/vmstat.c            |  1 +
 3 files changed, 14 insertions(+), 10 deletions(-)

diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index dcad2ab..ac1ea79 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -105,6 +105,7 @@
 enum zone_stat_item {
 	/* First 128 byte cacheline (assuming 64 bit words) */
 	NR_FREE_PAGES,
+        NR_ALLOC_BATCH,
 	NR_LRU_BASE,
 	NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */
 	NR_ACTIVE_ANON,		/*  "     "     "   "       "         */
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 0d7e9e9..6a95d39 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1554,6 +1554,7 @@ again:
   get_pageblock_migratetype(page));
  }
 
+ __mod_zone_page_state(zone, NR_ALLOC_BATCH, -(1 << order));
  __count_zone_vm_events(PGALLOC, zone, 1 << order);
  zone_statistics(preferred_zone, zone, gfp_flags);
  local_irq_restore(flags);
@@ -1927,7 +1928,7 @@ zonelist_scan:
  * fairness round-robin cycle of this zonelist.
  */
  if (alloc_flags & ALLOC_WMARK_LOW) {
- if (zone->alloc_batch <= 0)
+ if (zone_page_state(zone, NR_ALLOC_BATCH) <= 0)
  continue;
  if (zone_reclaim_mode &&
     !zone_local(preferred_zone, zone))
@@ -2039,8 +2040,7 @@ this_zone_full:
  goto zonelist_scan;
  }
 
- if (page) {
- zone->alloc_batch -= 1U << order;
+ if (page)
  /*
  * page->pfmemalloc is set when ALLOC_NO_WATERMARKS was
  * necessary to allocate the page. The expectation is
@@ -2049,7 +2049,6 @@ this_zone_full:
  * for !PFMEMALLOC purposes.
  */
  page->pfmemalloc = !!(alloc_flags & ALLOC_NO_WATERMARKS);
- }
 
  return page;
 }
@@ -2418,8 +2417,10 @@ static void prepare_slowpath(gfp_t gfp_mask, unsigned int order,
  */
  if (zone_reclaim_mode && !zone_local(preferred_zone, zone))
  continue;
- zone->alloc_batch = high_wmark_pages(zone) -
- low_wmark_pages(zone);
+ mod_zone_page_state(zone, NR_ALLOC_BATCH,
+    high_wmark_pages(zone) -
+    low_wmark_pages(zone) -
+    zone_page_state(zone, NR_ALLOC_BATCH));
  }
 }
 
@@ -4827,7 +4828,7 @@ static void __paginginit free_area_init_core(struct pglist_data *pgdat,
  zone->zone_pgdat = pgdat;
 
  /* For bootup, initialized properly in watermark setup */
- zone->alloc_batch = zone->managed_pages;
+ mod_zone_page_state(zone, NR_ALLOC_BATCH, zone->managed_pages);
 
  zone_pcp_init(zone);
  lruvec_init(&zone->lruvec);
@@ -5606,8 +5607,10 @@ static void __setup_per_zone_wmarks(void)
  zone->watermark[WMARK_LOW]  = min_wmark_pages(zone) + (tmp >> 2);
  zone->watermark[WMARK_HIGH] = min_wmark_pages(zone) + (tmp >> 1);
 
- zone->alloc_batch = high_wmark_pages(zone) -
- low_wmark_pages(zone);
+ __mod_zone_page_state(zone, NR_ALLOC_BATCH,
+      high_wmark_pages(zone) -
+      low_wmark_pages(zone) -
+      zone_page_state(zone, NR_ALLOC_BATCH));
 
  setup_zone_migrate_reserve(zone);
  spin_unlock_irqrestore(&zone->lock, flags);
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 87228c5..ba9e46b 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -704,6 +704,7 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,
 const char * const vmstat_text[] = {
  /* Zoned VM counters */
  "nr_free_pages",
+ "nr_alloc_batch",
  "nr_inactive_anon",
  "nr_active_anon",
  "nr_inactive_file",
